{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashankmutyala/ECG-to-SCG-signals-/blob/main/Copy_of_4x2_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mHIabIXWDydB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "metadata": {
        "id": "K9bUcpCsDnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install -U tensorflow\n",
        "!python -m pip show tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "!python -m pip show tensorflow\n",
        "!pip install tensorflow-addons\n",
        "!pip install -q tensorflow-addons\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install utils"
      ],
      "metadata": {
        "id": "388YACGMDte8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.fftpack import fft, ifft"
      ],
      "metadata": {
        "id": "PCTdBQmxuF47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyedflib\n",
        "!pip install padasip\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "0rSMXQtQD2_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cem7L7F-6SiB",
        "outputId": "88d277ce-2919-4cb2-c716-243569c32795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.16.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LayerNormalization, Input, Conv1D, Lambda, BatchNormalization, LeakyReLU, LSTM, Dense, Flatten, Layer, MultiHeadAttention, GlobalAveragePooling1D, UpSampling1D, Dropout, Embedding, Bidirectional, Normalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, legacy as legacy_optimizers\n",
        "from tensorflow.keras.losses import Huber\n",
        "from keras import regularizers, constraints, initializers, activations\n",
        "from keras.layers import TimeDistributed, InputSpec\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras_contrib.layers import InstanceNormalization\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, find_peaks\n",
        "import pyedflib\n",
        "import padasip as pa\n",
        "import optuna\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1VOk7-c5xF5"
      },
      "outputs": [],
      "source": [
        "fileNames = []\n",
        "fileName_str = []\n",
        "#Change the number here to read different files\n",
        "for i in range(10,11,1):\n",
        "  name_str = ''\n",
        "  if(i<10):\n",
        "    name_str = 'b00'\n",
        "  else:\n",
        "    name_str = 'b0'\n",
        "  fileNames.append(name_str+ str(i)+'.edf')\n",
        "  fileName_str.append(name_str+ str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_TPhcggPIG5"
      },
      "outputs": [],
      "source": [
        "original_scg = []\n",
        "original_ecg = []\n",
        "class DataUtils:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.fileNames = fileNames\n",
        "\n",
        "    def readData(self, sigNum, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/\"):\n",
        "        file_name = path + self.fileNames[sigNum]\n",
        "        f = pyedflib.EdfReader(file_name)\n",
        "        n = f.signals_in_file\n",
        "        signal_labels = f.getSignalLabels()\n",
        "        print(\"Reading file:: \",file_name)\n",
        "        print(\"different columns:: \",signal_labels)\n",
        "        print(\"total number of samples\",f.getNSamples())\n",
        "        abdECG = np.zeros((1, f.getNSamples()[0]))\n",
        "        scg = np.zeros((1, f.getNSamples()[0]))\n",
        "        scg[0, :] = f.readSignal(3)\n",
        "        scg = scale(scg, axis=1)\n",
        "        abdECG[0, :] = f.readSignal(0)\n",
        "        abdECG = scale(abdECG, axis=1)\n",
        "        print(\"before downsampling\",abdECG.shape[1])\n",
        "        abdECG = signal.resample(abdECG, int(abdECG.shape[1] / 5), axis=1)\n",
        "        scg = signal.resample(scg, int(scg.shape[1] / 5), axis=1)\n",
        "\n",
        "        print(\"after downsampling\",abdECG.shape[1])\n",
        "        '''function for ecg identification, using pan-tompkins algorithm'''\n",
        "        fs = []\n",
        "        fs.append(f.getSampleFrequency(0))\n",
        "\n",
        "        #changed the return sequence, so that converion will be from SCG to ECG\n",
        "        return  scg, abdECG, fs\n",
        "\n",
        "    def windowingSig(self, sig1, sig2, windowSize=15):\n",
        "        signalLen = sig2.shape[1]\n",
        "        signalsWindow1 = [sig1[:, int(i):int(i + windowSize)].transpose() for i in range(0, signalLen - windowSize, windowSize)]\n",
        "        signalsWindow2 = [sig2[:, int(i):int(i + windowSize)].transpose() for i in range(0, signalLen - windowSize, windowSize)]\n",
        "        print(\"SCG shape after windowing:: \",np.array(signalsWindow1).shape)\n",
        "        print(\"ECG shape after windowing:: \",np.array(signalsWindow2).shape)\n",
        "\n",
        "        return signalsWindow1, signalsWindow2\n",
        "\n",
        "    def adaptFilterOnSig(self, src, ref):\n",
        "        f = pa.filters.FilterNLMS(n=4, mu=0.1, w=\"random\")\n",
        "        for index, sig in enumerate(src):\n",
        "            try:\n",
        "                y, e, w = f.run(ref[index][:, 0], sig)\n",
        "                ref[index][:, 0] = e\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return ref\n",
        "\n",
        "    def calculateICA(self, sdSig, component=7):\n",
        "        ica = FastICA(n_components=component, max_iter=1000)\n",
        "        icaRes = []\n",
        "        for index, sig in enumerate(sdSig):\n",
        "            try:\n",
        "                icaSignal = np.array(ica.fit_transform(sig))\n",
        "                icaSignal = np.append(icaSignal, sig[:, range(2, 4)], axis=1)\n",
        "                icaRes.append(icaSignal)\n",
        "            except:\n",
        "                pass\n",
        "        return np.array(icaRes)\n",
        "\n",
        "    def createDelayRepetition(self, signal, numberDelay=4, delay=10):\n",
        "        signal = np.repeat(signal, numberDelay, axis=0)\n",
        "        for row in range(1, signal.shape[0]):\n",
        "            signal[row, :] = np.roll(signal[row, :], shift=delay * row)\n",
        "        return signal\n",
        "\n",
        "    def __butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
        "        nyq = 0.5 * fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "\n",
        "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=3, axis=1):\n",
        "        b, a = self.__butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = filtfilt(b, a, data, axis=axis)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMp0XeQhDxva"
      },
      "outputs": [],
      "source": [
        "'''function for ecg identification, using pan-tompkins algorithm'''\n",
        "\n",
        "def codeFn(ecg, fs):\n",
        "    nyquist = 0.5 * fs\n",
        "    low_cutoff = 5\n",
        "    high_cutoff = 15\n",
        "    b, a = butter(1, [low_cutoff/nyquist, high_cutoff/nyquist], btype='band')\n",
        "    ecg_filt = filtfilt(b, a, ecg)\n",
        "\n",
        "    b = np.array([1, 0, -1])\n",
        "    ecg_diff = np.convolve(ecg_filt, b, mode='same')\n",
        "    ecg_sq = ecg_diff ** 2\n",
        "\n",
        "    ma_len = int(0.08 * fs)\n",
        "    ecg_ma = np.convolve(ecg_sq, np.ones(ma_len)/ma_len, mode='same')\n",
        "\n",
        "    qrs_idx, _ = find_peaks(ecg_ma, distance=int(0.2 * fs), height=0.2 * np.max(ecg_ma))\n",
        "\n",
        "    return qrs_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyDUBs4I4keJ"
      },
      "outputs": [],
      "source": [
        "class TrainUtils:\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.dataUtils = DataUtils()\n",
        "\n",
        "    def prepareData(self, delay=5, path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001/b010.edf\"):\n",
        "        scgAll, ecg, fs = self.dataUtils.readData(0,path)\n",
        "        print(scgAll.shape)\n",
        "        scgAll = scgAll[range(1), :]\n",
        "        delayNum = scgAll.shape[0]\n",
        "        ecgAll = self.dataUtils.createDelayRepetition(ecg, delayNum, delay)\n",
        "        for i in range(1, len(fileNames)):\n",
        "            scg, ecg = self.dataUtils.readData(i,path)\n",
        "            print(\"Number of samples:: \",ecg.shape)\n",
        "            scg = scg[range(1), :]\n",
        "            ecgDelayed = self.dataUtils.createDelayRepetition(ecg, 1, delay)\n",
        "            scgAll = np.append(scgAll, scg, axis=1)\n",
        "            ecgAll = np.append(ecgAll, ecgDelayed, axis=1)\n",
        "        print(\"ECG all merged shape:: \", ecgAll.shape)\n",
        "\n",
        "        original_scg = scgAll\n",
        "        original_ecg = ecgAll\n",
        "        scgWindows, ecgWindows = self.dataUtils.windowingSig(scgAll, ecgAll, windowSize=1000)\n",
        "        return scgWindows, ecgWindows\n",
        "\n",
        "    def trainTestSplit(self, sig, label, trainPercent, shuffle=True):\n",
        "        print(\"Splitting into train and test:: \")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(sig, label, train_size=trainPercent, shuffle=False)\n",
        "        X_train = np.array(X_train)\n",
        "        X_test = np.array(X_test)\n",
        "        y_train = np.array(y_train)\n",
        "        y_test = np.array(y_test)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def window_the_data(self,data,window=32):\n",
        "      r = list()\n",
        "      for i, mat in enumerate(data):\n",
        "        s = 0\n",
        "        a = list()\n",
        "        print(data.shape)\n",
        "        while s + window < 256:\n",
        "          a.append(copy.deepcopy(mat[s : s + window].T))\n",
        "          s += 2\n",
        "        r.append(copy.deepcopy(a))\n",
        "\n",
        "      r = np.array(r)\n",
        "      return copy.deepcopy(r)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_signal_with_fft(signal, sample_rate=1000):\n",
        "    # Apply FFT to the signal\n",
        "    fft_signal = fft(signal)\n",
        "\n",
        "    # Frequency array\n",
        "    freqs = np.fft.fftfreq(len(signal), 1/sample_rate)\n",
        "\n",
        "    # Filter out frequencies outside the desired range (e.g., 0.5 to 50 Hz for ECG)\n",
        "    filtered_fft_signal = fft_signal.copy()\n",
        "    filtered_fft_signal[(freqs < 0.5) | (freqs > 50)] = 0\n",
        "\n",
        "    # Apply inverse FFT to get the filtered signal back in time domain\n",
        "    filtered_signal = ifft(filtered_fft_signal)\n",
        "\n",
        "    return np.real(filtered_signal)"
      ],
      "metadata": {
        "id": "9PK0pnzIuLNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWSGLeDSYotR"
      },
      "source": [
        "    **added 2 discriminators**\n",
        "\n",
        "---\n",
        "\n",
        "* line 103, 104 self.image shape c and d\n",
        "* line 124 - 140 discriminator c and d\n",
        "* line 151, 152 modified generators\n",
        "* line 167, 168 added fake_c and d\n",
        "* line 286 and 287 added batches C and D\n",
        "* line 299-302 added imgs_c, imgs_d\n",
        "* line 310, 311 fake_c and d\n",
        "* line 315, 316 added reconstr_c and d\n",
        "* line 324 added img_c and d for g_loss\n",
        "* line 326-332 added dC and dD loss real and fake\n",
        "* line 335 modified d_loss\n",
        "* line 376-384 added fake_c and d, reconstr_c and d\n",
        "* line 394 added imgs_C, fake_D, reconstr_C, imgs_D, fake_C, reconstr_D to gen_imgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAw_aBmpNpv0"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, rate=0.01):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-4)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs * attn_output)\n",
        "        return out1\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        base_config = super(TransformerBlock, self).get_config()\n",
        "        return dict(list(base_config.items()))\n",
        "\n",
        "\n",
        "\n",
        "def batch_creation(x_train, y_train, batch_size, batch_idx):\n",
        "    batchA = x_train[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "    batchB = y_train[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "    return batchA, batchB\n",
        "\n",
        "class CycleGAN:\n",
        "    def __init__(self, row, col):\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                # Restrict TensorFlow to only use the fourth GPU\n",
        "                tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "                # Currently, memory growth needs to be the same across GPUs\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "            except RuntimeError as e:\n",
        "                # Memory growth must be set before GPUs have been initialized\n",
        "                print(e)\n",
        "        self.dataUtils = DataUtils()\n",
        "        self.img_rows = row\n",
        "        self.img_cols = col\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols)\n",
        "        self.img_shape_b = (self.img_rows, 1)\n",
        "        self.dataset_name = 'ECG2FECG'\n",
        "\n",
        "        # Calculate output shape of D (PatchGAN)\n",
        "        patch = int(self.img_rows / 2 ** 4)\n",
        "        self.disc_patch = (self.img_rows, 1)\n",
        "\n",
        "        # Number of filters in the first layer of G and D\n",
        "        self.gf = 6\n",
        "        self.df = 13\n",
        "\n",
        "        # Loss weights\n",
        "        self.lambda_cycle = 4.0  # Cycle-consistency loss\n",
        "        self.lambda_id = 0.01 * self.lambda_cycle  # Identity loss\n",
        "\n",
        "        # optimizer = Adam()\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "        # Build and compile the discriminators\n",
        "        self.d_A1 = self.build_discriminator(self.img_shape)\n",
        "        # self.d_A2 = self.build_discriminator(self.img_shape)\n",
        "        self.d_A2 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B1 = self.build_discriminator(self.img_shape_b)\n",
        "        self.d_B2 = self.build_discriminator(self.img_shape)\n",
        "        # self.d_B2 = self.build_discriminator(self.img_shape_b)\n",
        "\n",
        "        # Build the generators\n",
        "        self.g_AB = self.build_generator(self.img_shape)\n",
        "        self.g_BA = self.build_generator(self.img_shape_b)\n",
        "\n",
        "        # Compile discriminators\n",
        "        self.d_A1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_A2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B1.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "        self.d_B2.compile(loss='MSE', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        # Input images from both domains\n",
        "        img_A = Input(shape=self.img_shape)\n",
        "        img_B = Input(shape=self.img_shape_b)\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB(img_A)\n",
        "        fake_A = self.g_BA(img_B)\n",
        "\n",
        "        # Translate images back to original domain\n",
        "        reconstr_A = self.g_BA(fake_B)\n",
        "        reconstr_B = self.g_AB(fake_A)\n",
        "        #print(\"1\")\n",
        "        # Identity mapping of images\n",
        "        img_A_id = self.g_BA(img_A)\n",
        "        img_B_id = self.g_AB(img_B)\n",
        "\n",
        "        self.d_A1.trainable = True\n",
        "        self.d_A2.trainable = True\n",
        "        self.d_B1.trainable = True\n",
        "        self.d_B2.trainable = True\n",
        "\n",
        "        # Discriminators determine validity of translated images\n",
        "        valid_A1 = self.d_A1(fake_A)\n",
        "        valid_A2 = self.d_A2(fake_A)\n",
        "        valid_B1 = self.d_B1(fake_B)\n",
        "        valid_B2 = self.d_B2(fake_B)\n",
        "\n",
        "        # # Combined model trains generators to fool discriminators\n",
        "        self.combined = Model(inputs=[img_A, img_B],\n",
        "                              outputs=[valid_A1, valid_B1, valid_A2, valid_B2,\n",
        "                                       fake_B, fake_A,\n",
        "                                       reconstr_A, reconstr_B])\n",
        "\n",
        "        # Compile the combined model\n",
        "        self.combined.compile(loss=['huber_loss', 'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss', 'huber_loss'],\n",
        "                              loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle, self.lambda_id, self.lambda_id],\n",
        "                              optimizer=optimizer)\n",
        "\n",
        "    # Define custom loss\n",
        "    def custom_loss(self):\n",
        "\n",
        "        # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
        "        def loss(y_true, y_pred):\n",
        "            return K.mean(y_true * K.log(y_true / y_pred + K.epsilon()))\n",
        "\n",
        "        # Return a function\n",
        "        return loss\n",
        "\n",
        "    def build_generator(self,img_shape):\n",
        "        \"\"\"U-Net Generator\"\"\"\n",
        "        print(\"In build generator\")\n",
        "        def conv1DWithSINE(layer_input, filters, f_size=60):\n",
        "            \"\"\"Layers used during downsampling\"\"\"\n",
        "            # d = Conv1D(filters, kernel_size=f_size, padding='same', activation='sigmoid')(layer_input)\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same', activation='LeakyReLU')(layer_input)\n",
        "            d = InstanceNormalization()(d)\n",
        "            d = InstanceNormalization()(d)\n",
        "            d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "    def multiply(x):\n",
        "        mask,image  = x\n",
        "        return image* K.clip(mask,0.8,1)\n",
        "\n",
        "        input = Input(shape=img_shape)\n",
        "        print(\"Input shape:: \", input.shape)\n",
        "        # value = conv1DWithSINE(input, input.shape[2], f_size=30)\n",
        "        value = conv1DWithSINE(input, 30, f_size=60)\n",
        "        print(\" shape after conv1d :: \", value.shape)\n",
        "\n",
        "        print(\"apply attention.\")\n",
        "        att = TransformerBlock(embed_dim=input.shape[1], num_heads=2)(value)\n",
        "        print(\"shape after attention:: \",att.shape )\n",
        "        att = Normalization(axis=1)(att)\n",
        "        print(\"shape after normalizing attention:: \",att.shape )\n",
        "\n",
        "        remainedInput = Lambda(multiply)([att, value])\n",
        "        print(\"shape after apply lambda:: \",remainedInput.shape )\n",
        "\n",
        "        output_img = conv1DWithSINE(remainedInput, 17, f_size=240)#60\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)#60\n",
        "        output_img = conv1DWithSINE(output_img, 13, f_size=240)#60\n",
        "        output_img = conv1DWithSINE(output_img, 1, f_size=1)\n",
        "\n",
        "        return Model(input, output_img)\n",
        "\n",
        "    def build_discriminator(self,img_shape):\n",
        "        print(\"In building discriminator\")\n",
        "        def d_layer(layer_input, filters, f_size=33, normalization=True):\n",
        "            \"\"\"Discriminator layer\"\"\"\n",
        "            # d = Conv1D(filters, kernel_size=f_size, padding='same',activation='sigmoid')(layer_input)\n",
        "            d = Conv1D(filters, kernel_size=f_size, padding='same',activation=LeakyReLU())(layer_input)\n",
        "            if normalization:\n",
        "                d = InstanceNormalization()(d)\n",
        "                d = BatchNormalization()(d)\n",
        "            return d\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        print(\"input shape:: \",img.shape)\n",
        "        d1 = d_layer(img, self.df)\n",
        "        print(\"input shape after first conv1d:: \",d1.shape)\n",
        "        d2 = d_layer(d1, 13)\n",
        "        print(\"input shape after second conv1d:: \",d2.shape)\n",
        "        d3 = d_layer(d2, 13)\n",
        "        print(\"input shape after third conv1d:: \",d3.shape)\n",
        "        d4 = d_layer(d3, 13)\n",
        "        print(\"input shape after fourth conv1d:: \",d3.shape)\n",
        "        d5 = d_layer(d4, 13)\n",
        "        print(\"input shape after fifth conv1d:: \",d3.shape)\n",
        "        d6 = d_layer(d5, 13)\n",
        "        print(\"input shape after sixth conv1d:: \",d3.shape)\n",
        "        validity = d_layer(d6, 1)\n",
        "        # validity = d_layer(d3, img.shape[1])\n",
        "        print(\"final shape after 6th layer:: \",validity.shape)\n",
        "\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, x_train, y_train, epochs, batch_size=1, sample_interval=5):\n",
        "        print(\"Num Samples\", x_train.shape[0])\n",
        "        print(\"In train:: \")\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        # Adversarial loss ground truths\n",
        "        valid = np.ones((batch_size,) + self.disc_patch)\n",
        "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx in range(x_train.shape[0] // batch_size):\n",
        "                batchA, batchB = batch_creation(x_train, y_train, batch_size, batch_idx)\n",
        "                batchA = np.asarray(batchA)\n",
        "                batchB = np.asarray(batchB)\n",
        "\n",
        "                imgs_A = batchA\n",
        "                imgs_B = batchB\n",
        "\n",
        "                imgs_B = np.reshape(imgs_B, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "                imgs_A = np.reshape(imgs_A, (-1, x_train.shape[1], x_train.shape[2]))\n",
        "\n",
        "                # ----------------------\n",
        "                #  Train Discriminators\n",
        "                # ----------------------\n",
        "\n",
        "                # Translate images to the other domain\n",
        "                fake_B = self.g_AB.predict(imgs_A)\n",
        "                fake_A = self.g_BA.predict(imgs_B)\n",
        "\n",
        "                # Translate images back to original domain\n",
        "                reconstr_A = self.g_BA.predict(fake_B)\n",
        "                reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "                # Train the discriminators (original images = real / translated = Fake)\n",
        "                dA_loss_real = self.d_A1.train_on_batch(imgs_A, valid)\n",
        "                dA_loss_fake = self.d_A1.train_on_batch(fake_A, fake)\n",
        "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
        "\n",
        "                dB_loss_real = self.d_B1.train_on_batch(imgs_B, valid)#b\n",
        "                dB_loss_fake = self.d_B1.train_on_batch(fake_B, fake)\n",
        "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
        "\n",
        "                dC_loss_real = self.d_A2.train_on_batch(imgs_B, valid)#a\n",
        "                dC_loss_fake = self.d_A2.train_on_batch(fake_B, fake)\n",
        "                dC_loss = 0.5 * np.add(dC_loss_real, dC_loss_fake)\n",
        "\n",
        "                dD_loss_real = self.d_B2.train_on_batch(imgs_A, valid)#a\n",
        "                dD_loss_fake = self.d_B2.train_on_batch(fake_A, fake)\n",
        "                dD_loss = 0.5 * np.add(dD_loss_real, dD_loss_fake)\n",
        "                d_loss = 0.25 * (np.add(np.add(dA_loss, dB_loss), np.add(dC_loss, dD_loss)))\n",
        "\n",
        "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
        "                                                      [valid, valid, valid, valid,\n",
        "                                                       imgs_B, imgs_A,\n",
        "                                                       reconstr_A, reconstr_B])\n",
        "                elapsed_time = datetime.datetime.now() - start_time\n",
        "\n",
        "                # Plot the progress\n",
        "                print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
        "                      % (epoch, epochs,\n",
        "                         batch_idx, 1,\n",
        "                         d_loss[0], 100 * d_loss[1],\n",
        "                         g_loss[0],\n",
        "                         np.mean(g_loss[1:3]),\n",
        "                         np.mean(g_loss[3:5]),\n",
        "                         np.mean(g_loss[5:6]),\n",
        "                         elapsed_time))\n",
        "\n",
        "                # If at save interval => save generated image samples\n",
        "                if batch_idx % sample_interval == 0:\n",
        "                    self.sample_images(epoch, batch_idx, imgs_A, imgs_B)\n",
        "                    self.g_AB.save(\"ECG2FECG.h5\", overwrite=True)\n",
        "                    self.g_BA.save(\"FECG2ECG.h5\", overwrite=True)\n",
        "\n",
        "    def sample_images(self, epoch, batch_idx, imgs_A, imgs_B):\n",
        "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
        "        r, c = 2, 3\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_B = self.g_AB.predict(imgs_A)\n",
        "        fake_A = self.g_BA.predict(imgs_B)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.g_BA.predict(fake_B)\n",
        "        reconstr_B = self.g_AB.predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "\n",
        "        try:\n",
        "            for i in range(r):\n",
        "                for j in range(c):\n",
        "                    for bias in range(1):\n",
        "                        if np.max(gen_imgs[cnt][:, bias]) != 0:\n",
        "                            gen_imgs[cnt][:, bias] = gen_imgs[cnt][:, bias] / np.max(gen_imgs[cnt][:, bias])\n",
        "                        axs[i, j].plot(gen_imgs[cnt][:, bias] + bias)\n",
        "                    axs[i, j].set_title(titles[j])\n",
        "                    cnt += 1\n",
        "            plt.close()\n",
        "        except:\n",
        "            pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWEa6jqtQECU"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder():\n",
        "\n",
        "    def __init__(self, units, output_dim,\n",
        "                 activation='tanh',\n",
        "                 return_probabilities=False,\n",
        "                 name='AttentionDecoder',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 recurrent_initializer='orthogonal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Implements an AttentionDecoder that takes in a sequence encoded by an\n",
        "        encoder and outputs the decoded states\n",
        "        :param units: dimension of the hidden state and the attention matrices\n",
        "        :param output_dim: the number of labels in the output space\n",
        "        references:\n",
        "            Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio.\n",
        "            \"Neural machine translation by jointly learning to align and translate.\"\n",
        "            arXiv preprint arXiv:1409.0473 (2014).\n",
        "        \"\"\"\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "        self.return_probabilities = return_probabilities\n",
        "        self.activation = activations.get(activation)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.recurrent_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "\n",
        "        super(AttentionDecoder, self).__init__(**kwargs)\n",
        "        self.name = name\n",
        "        self.return_sequences = True  # must return sequences\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "          See Appendix 2 of Bahdanau 2014, arXiv:1409.0473\n",
        "          for model details that correspond to the matrices here.\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size, self.timesteps, self.input_dim = input_shape\n",
        "\n",
        "        if self.stateful:\n",
        "            super(AttentionDecoder, self).reset_states()\n",
        "\n",
        "        self.states = [None, None]  # y, s\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for creating the context vector\n",
        "        \"\"\"\n",
        "\n",
        "        self.V_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='V_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.W_a = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='W_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.U_a = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='U_a',\n",
        "                                   initializer=self.kernel_initializer,\n",
        "                                   regularizer=self.kernel_regularizer,\n",
        "                                   constraint=self.kernel_constraint)\n",
        "        self.b_a = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_a',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the r (reset) gate\n",
        "        \"\"\"\n",
        "        self.C_r = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_r = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_r = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_r',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_r = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_r',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        \"\"\"\n",
        "            Matrices for the z (update) gate\n",
        "        \"\"\"\n",
        "        self.C_z = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_z = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_z = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_z',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_z = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_z',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for the proposal\n",
        "        \"\"\"\n",
        "        self.C_p = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='C_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_p = self.add_weight(shape=(self.units, self.units),\n",
        "                                   name='U_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_p = self.add_weight(shape=(self.output_dim, self.units),\n",
        "                                   name='W_p',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_p = self.add_weight(shape=(self.units,),\n",
        "                                   name='b_p',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "        \"\"\"\n",
        "            Matrices for making the final prediction vector\n",
        "        \"\"\"\n",
        "        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim),\n",
        "                                   name='C_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.output_dim),\n",
        "                                   name='U_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim),\n",
        "                                   name='W_o',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "        self.b_o = self.add_weight(shape=(self.output_dim,),\n",
        "                                   name='b_o',\n",
        "                                   initializer=self.bias_initializer,\n",
        "                                   regularizer=self.bias_regularizer,\n",
        "                                   constraint=self.bias_constraint)\n",
        "\n",
        "        # For creating the initial state:\n",
        "        self.W_s = self.add_weight(shape=(self.input_dim, self.units),\n",
        "                                   name='W_s',\n",
        "                                   initializer=self.recurrent_initializer,\n",
        "                                   regularizer=self.recurrent_regularizer,\n",
        "                                   constraint=self.recurrent_constraint)\n",
        "\n",
        "        self.input_spec = [\n",
        "            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n",
        "        self.built = True\n",
        "\n",
        "    def _time_distributed_dense(self,x, w, b=None, dropout=None,\n",
        "                                input_dim=None, output_dim=None,\n",
        "                                timesteps=None, training=None):\n",
        "        \"\"\"Apply `y . w + b` for every temporal slice y of x.\n",
        "        # Arguments\n",
        "            x: input tensor.\n",
        "            w: weight matrix.\n",
        "            b: optional bias vector.\n",
        "            dropout: wether to apply dropout (same dropout mask\n",
        "                for every temporal slice of the input).\n",
        "            input_dim: integer; optional dimensionality of the input.\n",
        "            output_dim: integer; optional dimensionality of the output.\n",
        "            timesteps: integer; optional number of timesteps.\n",
        "            training: training phase tensor or boolean.\n",
        "        # Returns\n",
        "            Output tensor.\n",
        "        \"\"\"\n",
        "        if not input_dim:\n",
        "            input_dim = K.shape(x)[2]\n",
        "        if not timesteps:\n",
        "            timesteps = K.shape(x)[1]\n",
        "        if not output_dim:\n",
        "            output_dim = K.shape(w)[1]\n",
        "\n",
        "        if dropout is not None and 0. < dropout < 1.:\n",
        "            # apply the same dropout pattern at every timestep\n",
        "            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n",
        "            dropout_matrix = K.dropout(ones, dropout)\n",
        "            expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n",
        "            x = K.in_train_phase(x * expanded_dropout_matrix, x, training=training)\n",
        "\n",
        "        # collapse time dimension and batch dimension together\n",
        "        x = K.reshape(x, (-1, input_dim))\n",
        "        x = K.dot(x, w)\n",
        "        if b is not None:\n",
        "            x = K.bias_add(x, b)\n",
        "        # reshape to 3D tensor\n",
        "        if K.backend() == 'tensorflow':\n",
        "            x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n",
        "            x.set_shape([None, None, output_dim])\n",
        "        else:\n",
        "            x = K.reshape(x, (-1, timesteps, output_dim))\n",
        "        return x\n",
        "\n",
        "    def call(self, x):\n",
        "        # store the whole sequence so we can \"attend\" to it at each timestep\n",
        "        self.x_seq = x\n",
        "\n",
        "        # apply the a dense layer over the time dimension of the sequence\n",
        "        # do it here because it doesn't depend on any previous steps\n",
        "        # thefore we can save computation time:\n",
        "        self._uxpb = self._time_distributed_dense(self.x_seq, self.U_a, b=self.b_a,\n",
        "                                             input_dim=self.input_dim,\n",
        "                                             timesteps=self.timesteps,\n",
        "                                             output_dim=self.units)\n",
        "\n",
        "        return super(AttentionDecoder, self).call(x, )\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        # apply the matrix on the first time step to get the initial s0.\n",
        "        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n",
        "\n",
        "        # from keras.layers.recurrent to initialize a vector of (batchsize,\n",
        "        # output_dim)\n",
        "        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n",
        "        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n",
        "        y0 = K.expand_dims(y0)  # (samples, 1)\n",
        "        y0 = K.tile(y0, [1, self.output_dim])\n",
        "\n",
        "        return [y0, s0]\n",
        "\n",
        "    def step(self, x, states):\n",
        "\n",
        "        ytm, stm = states\n",
        "\n",
        "        # repeat the hidden state to the length of the sequence\n",
        "        _stm = K.repeat(stm, self.timesteps)\n",
        "\n",
        "        # now multiplty the weight matrix with the repeated hidden state\n",
        "        _Wxstm = K.dot(_stm, self.W_a)\n",
        "\n",
        "        # calculate the attention probabilities\n",
        "        # this relates how much other timesteps contributed to this one.\n",
        "        et = K.dot(activations.tanh(_Wxstm + self._uxpb),\n",
        "                   K.expand_dims(self.V_a))\n",
        "        at = K.exp(et)\n",
        "        at_sum = K.sum(at, axis=1)\n",
        "        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n",
        "        at /= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n",
        "\n",
        "        # calculate the context vector\n",
        "        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n",
        "        # ~~~> calculate new hidden state\n",
        "        # first calculate the \"r\" gate:\n",
        "\n",
        "        rt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_r)\n",
        "            + K.dot(stm, self.U_r)\n",
        "            + K.dot(context, self.C_r)\n",
        "            + self.b_r)\n",
        "\n",
        "        # now calculate the \"z\" gate\n",
        "        zt = activations.sigmoid(\n",
        "            K.dot(ytm, self.W_z)\n",
        "            + K.dot(stm, self.U_z)\n",
        "            + K.dot(context, self.C_z)\n",
        "            + self.b_z)\n",
        "\n",
        "        # calculate the proposal hidden state:\n",
        "        s_tp = activations.tanh(\n",
        "            K.dot(ytm, self.W_p)\n",
        "            + K.dot((rt * stm), self.U_p)\n",
        "            + K.dot(context, self.C_p)\n",
        "            + self.b_p)\n",
        "\n",
        "        # new hidden state:\n",
        "        st = (1 - zt) * stm + zt * s_tp\n",
        "\n",
        "        yt = activations.softmax(\n",
        "            K.dot(ytm, self.W_o)\n",
        "            + K.dot(stm, self.U_o)\n",
        "            + K.dot(context, self.C_o)\n",
        "            + self.b_o)\n",
        "\n",
        "        if self.return_probabilities:\n",
        "            return at, [yt, st]\n",
        "        else:\n",
        "            return yt, [yt, st]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "            For Keras internal compatability checking\n",
        "        \"\"\"\n",
        "        if self.return_probabilities:\n",
        "            return (None, self.timesteps, self.timesteps)\n",
        "        else:\n",
        "            return (None, self.timesteps, self.output_dim)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "            For rebuilding models on load time.\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'units': self.units,\n",
        "            'return_probabilities': self.return_probabilities\n",
        "        }\n",
        "        base_config = super(AttentionDecoder, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxqkhBIvFiOg"
      },
      "source": [
        "#Hyper Parameter tuning starts here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5-PxrDQFsra"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#Grid search CV\n",
        "def create_model(learning_rate=0.01):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid = {\n",
        "    'batch_size': [10, 20, 40],\n",
        "    'epochs': [50, 100, 150],\n",
        "    'learning_rate': [0.001, 0.01, 0.1]\n",
        "}\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJvOq7JzHr8j"
      },
      "source": [
        "#Hyperparameter tuning ends here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXdJaaSpNkfO"
      },
      "outputs": [],
      "source": [
        "\n",
        "from unittest import TestCase\n",
        "import numpy as np\n",
        "\n",
        "# from utils.TrainUtils import TrainUtil\n",
        "# from utils.TrainUtils import TrainUtils\n",
        "# from deeplearning.CycleGAN import CycleGAN\n",
        "\n",
        "\n",
        "class TestCycleGAN(TestCase):\n",
        "\n",
        "    def __init__(self, methodName: str = ...) -> None:\n",
        "        super().__init__(methodName)\n",
        "        self.trainUtils = TrainUtils()\n",
        "\n",
        "    def test_trainSignal(self,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/b010.edf\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/\")\n",
        "        print(\"ecgWindows:: \",np.array(ecgWindows).shape)\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        X_train = np.reshape(X_train, [-1, X_train.shape[1], X_train.shape[2]])\n",
        "        # X_test = np.reshape(X_test, [-1, X_test.shape[1], X_test.shape[2], 1])\n",
        "        Y_train = np.reshape(Y_train, [-1, Y_train.shape[1], Y_train.shape[2]])\n",
        "\n",
        "        print(\"Shape of x train and y train:: \",X_train.shape, Y_train.shape)\n",
        "        print(\"Shape of x test and y test:: \",X_test.shape, y_test.shape)\n",
        "        # y_test = np.reshape(Y_test, [-1, Y_test.shape[1], Y_test.shape[2], 1])\n",
        "        cycleGAN = CycleGAN(X_train.shape[1], X_train.shape[2])\n",
        "        print(\"model instantiated\")\n",
        "        cycleGAN.train(x_train=X_train, y_train=Y_train, epochs=1)\n",
        "        return cycleGAN, X_test, y_test\n",
        "    def divide_test_train(self,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)/b010.edf\"):\n",
        "        scgWindows, ecgWindows = self.trainUtils.prepareData(delay=2,path=\"/content/drive/MyDrive/drive-download-20230428T081947Z-001 (1)\")\n",
        "        X_train, X_test, Y_train, y_test = self.trainUtils.trainTestSplit(scgWindows, ecgWindows, 0.75)\n",
        "        return X_train, X_test, Y_train, y_test\n",
        "#model = TestCycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOqLtJow4Sbk"
      },
      "outputs": [],
      "source": [
        "model = TestCycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nk72Kbz7AxB"
      },
      "outputs": [],
      "source": [
        "# @tf.autograph.experimental.do_not_convert\n",
        "cycleGan, X_test, y_test = model.test_trainSignal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9qXAupK8yP"
      },
      "outputs": [],
      "source": [
        "#model = TestcycleGAN(\"test_trainSignal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_EyygfkiS9D"
      },
      "outputs": [],
      "source": [
        "x_g_AB =cycleGan.g_AB.predict(X_test)\n",
        "#x_g_BA = cycleGan.g_BA.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOoeJ1miu81V"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# a = np.array(y_test.flatten())\n",
        "# b = x_g_AB.flatten()\n",
        "# print(len(a), len(b))\n",
        "# df1 = pd.DataFrame({ \"ECG Original\" : a})\n",
        "# df2 = pd.DataFrame({ \"ECG Predicted\" : b})\n",
        "# df1.to_csv(\"/content/drive/MyDrive/ECG_data/original_ECG_1.csv\", index=False)\n",
        "# df2.to_csv(\"/content/drive/MyDrive/ECG_data/generated_ECG_1.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# a = np.array(original_scg)\n",
        "a = np.array(y_test.flatten())\n",
        "b = x_g_AB.flatten()\n",
        "print(len(a), len(b))\n",
        "df1 = pd.DataFrame({ \"ECG Original\" : a})\n",
        "df2 = pd.DataFrame({ \"ECG Predicted\" : b})\n",
        "df1.to_csv(\"original_ECG_10.csv\", index=False)\n",
        "#df2.to_csv(\"/content/drive/MyDrive/ECG_data/generated_ECG_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcUbWMkcVTQA"
      },
      "outputs": [],
      "source": [
        "!pip install heartpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpOjSUASVWzH"
      },
      "outputs": [],
      "source": [
        "import heartpy as hp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sample_rate = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ld_q3I2VaDl"
      },
      "outputs": [],
      "source": [
        "data = hp.get_data('/content/original_ECG_10.csv')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(data)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwCoAPRaVgN7"
      },
      "outputs": [],
      "source": [
        "#run analysis\n",
        "wd, m = hp.process(data[1:], sample_rate)\n",
        "\n",
        "#visualise in plot of custom size\n",
        "plt.figure(figsize=(12,4))\n",
        "hp.plotter(wd, m)\n",
        "#print(len(wd['hr']))\n",
        "#print(wd.keys())\n",
        "#print(wd['RR_list'])\n",
        "#heart rates\n",
        "hrs = 60000/wd['RR_list']\n",
        "print(\"heart rates: \",hrs)\n",
        "#display computed measures\n",
        "for measure in m.keys():\n",
        "    print('%s: %f' %(measure, m[measure]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRmQp2UPVjL0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df2 = pd.DataFrame({ \"Heart Rate\" : np.array(hrs)})\n",
        "df2.to_csv(\"hr_original_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMobxa04W9_2"
      },
      "outputs": [],
      "source": [
        "# data = hp.get_data('original_ECG_15.csv')\n",
        "\n",
        "# plt.figure(figsize=(12,4))\n",
        "# plt.plot(data)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qofWbnXE6x"
      },
      "outputs": [],
      "source": [
        "#run analysis\n",
        "wd, m = hp.process(data[1:], sample_rate)\n",
        "\n",
        "#visualise in plot of custom size\n",
        "plt.figure(figsize=(12,4))\n",
        "hp.plotter(wd, m)\n",
        "#print(len(wd['hr']))\n",
        "#print(wd.keys())\n",
        "#print(wd['RR_list'])\n",
        "#heart rates\n",
        "hrs = 60000/wd['RR_list']\n",
        "print(\"heart rates: \",hrs)\n",
        "#display computed measures\n",
        "for measure in m.keys():\n",
        "    print('%s: %f' %(measure, m[measure]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGAOERatXImS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# df2 = pd.DataFrame({ \"Heart Rate\" : np.array(hrs)})\n",
        "# df2.to_csv(\"/content/drive/MyDrive/ECG_data/hr_original_12.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9FkK6KzUdUK"
      },
      "outputs": [],
      "source": [
        "# data = hp.get_data('/content/drive/MyDrive/ECG_data/hr_original_10.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md_oDmlrAEfa"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data)):\n",
        "  if data[i]>500:\n",
        "    data[i]=data[i]/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h-0_RjqAHTX"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# df2 = pd.DataFrame({ \"Heart Rate\" : np.array(data)})\n",
        "# df2.to_csv(\"/content/drive/MyDrive/ECG_data/hr_original_10.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-cQFHJz_Nrf"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_g_AB[1])\n",
        "plt.plot(y_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PWaTgxKUEQg"
      },
      "outputs": [],
      "source": [
        "#X_test_in = []\n",
        "#x_g_AB_in = []\n",
        "#inverse differencing\n",
        "#for i in range(len(X_test)):\n",
        "#  X_test_in.append((np.concatenate(([X_test[i][0]], X_test[i])).cumsum())[:x_g_AB.shape[1]])\n",
        "#  x_g_AB_in.append((np.concatenate(([x_g_AB[i][0]], x_g_AB[i])).cumsum())[:x_g_AB.shape[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvGRVswC-0aO"
      },
      "outputs": [],
      "source": [
        "# len(x_g_AB_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v2SVaik_I2z"
      },
      "outputs": [],
      "source": [
        "# len(x_g_AB_in[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8CBh3HxUW1t"
      },
      "outputs": [],
      "source": [
        "# x_g_AB = np.array(x_g_AB_in).reshape(x_g_AB.shape[0],x_g_AB.shape[1],1)\n",
        "# X_test = np.array(X_test_in).reshape(X_test.shape[0],X_test.shape[1],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPHBd5X9VDNy"
      },
      "outputs": [],
      "source": [
        "#Chnaged\n",
        "def smooth(x, window_len=11, window='hanning'):\n",
        "    if window_len < 3:\n",
        "        return x\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-2:-window_len-1:-1]]\n",
        "    if window == 'flat':  # moving average\n",
        "        w = np.ones(window_len, 'd')\n",
        "    else:\n",
        "        w = eval('np.' + window + '(window_len)')\n",
        "    y = np.convolve(w / w.sum(), s, mode='valid')\n",
        "    return y[(window_len//2-1):-(window_len//2)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change\n",
        "# Original reshaping part\n",
        "x_g_AB_reshaped = x_g_AB.reshape(x_g_AB.shape[0], x_g_AB.shape[1])\n",
        "x_g_AB_reshaped.shape\n",
        "\n",
        "# Apply FFT-based preprocessing\n",
        "x_g_AB_preprocessed = []\n",
        "for i in range(len(x_g_AB_reshaped)):\n",
        "    preprocessed = preprocess_signal_with_fft(x_g_AB_reshaped[i])\n",
        "    x_g_AB_preprocessed.append(preprocessed)\n",
        "\n",
        "x_g_AB_preprocessed = np.array(x_g_AB_preprocessed)\n"
      ],
      "metadata": {
        "id": "6gQnAwapuYFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changed\n",
        "def extract_fft_features(signal, sample_rate=1000):\n",
        "    # Apply FFT to the signal\n",
        "    fft_signal = fft(signal)\n",
        "\n",
        "    # Compute the magnitude of the FFT\n",
        "    magnitude = np.abs(fft_signal)\n",
        "\n",
        "    # Select a subset of the frequency components as features (e.g., the first 50 components)\n",
        "    features = magnitude[:50]\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "F_usqkAeuh2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrJEw9AjM2eI"
      },
      "outputs": [],
      "source": [
        "#          Testing functions\n",
        "#################################################################################\n",
        "def rmse(targets, predictions):\n",
        "    return np.sqrt(np.mean((targets-predictions)**2))\n",
        "\n",
        "\n",
        "def prd(targets, predictions):\n",
        "    s1 = np.sum((targets-predictions)**2)\n",
        "    s2 = np.sum(targets**2)\n",
        "    return np.sqrt(s1 / s2 * 100)\n",
        "\n",
        "\n",
        "def mmd(targets, predictions):\n",
        "    mmd_stat = MMDStatistic(400, 400)\n",
        "    sample_target = torch.from_numpy(targets.numpy().reshape((400,1)))\n",
        "    sample_pred = torch.from_numpy(predictions.numpy().reshape((400,1)))\n",
        "\n",
        "    stat = mmd_stat(sample_target, sample_pred, [1.])\n",
        "    return(stat.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxYLtf_JYGyZ"
      },
      "outputs": [],
      "source": [
        "x_g_AB_smoothed = np.array(x_g_AB_smoothed)\n",
        "x_g_AB_smoothed = x_g_AB_smoothed.reshape(x_g_AB_smoothed.shape[0],x_g_AB_smoothed.shape[1],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWazhMXzrh8P"
      },
      "outputs": [],
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k8C5LdhrgP0"
      },
      "outputs": [],
      "source": [
        "x_g_AB_smoothed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCtZY8RUQtjf"
      },
      "outputs": [],
      "source": [
        "#for i in range(len(x_g_AB)):\n",
        "rmse_arr = rmse(y_test,x_g_AB)\n",
        "print(rmse_arr)\n",
        "rmse_smoothed = rmse(y_test,x_g_AB_smoothed)\n",
        "print(\"smoothed\",rmse_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6pV9wzdlTeS"
      },
      "outputs": [],
      "source": [
        "rmse_all =[]\n",
        "rmse_all_smoothened =[]\n",
        "for i in range(len(x_g_AB)):\n",
        "  rmse_all.append(rmse(y_test[i],x_g_AB[i]))\n",
        "  rmse_all_smoothened.append(rmse(y_test[i],x_g_AB_smoothed[i]))\n",
        "\n",
        "rmse_all_mean = np.array(rmse_all).mean()\n",
        "print(rmse_all_mean)\n",
        "rmse_all_smoothed_mean = np.array(rmse_all_smoothened).mean()\n",
        "print(\"smoothed\",rmse_all_smoothed_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wdDoH7eQ6Hh"
      },
      "outputs": [],
      "source": [
        "\n",
        "prd_arr = prd(y_test,x_g_AB)\n",
        "print(prd_arr)\n",
        "prd_arr_smoothed = prd(y_test,x_g_AB_smoothed)\n",
        "print(\"smoothed\",prd_arr_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHBGblEDuehI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mae_arr = mean_absolute_error(y_test.flatten(),x_g_AB.flatten())\n",
        "print(mae_arr)\n",
        "mae_arr_smoothed = mean_absolute_error(y_test.flatten(),x_g_AB_smoothed.flatten())\n",
        "print(\"smoothed\",mae_arr_smoothed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQVy4CB10gIT"
      },
      "outputs": [],
      "source": [
        "def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):\n",
        "    '''\n",
        "    Peak detection algorithm using cross corrrelation and threshold\n",
        "    '''\n",
        "    if qrs_filter is None:\n",
        "        # create default qrs filter, which is just a part of the sine function\n",
        "        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)\n",
        "        qrs_filter = np.sin(t)\n",
        "\n",
        "    # normalize data\n",
        "    ecg_signal = (ecg_signal - ecg_signal.mean()) / ecg_signal.std()\n",
        "\n",
        "    # calculate cross correlation\n",
        "    similarity = np.correlate(ecg_signal, qrs_filter, mode=\"same\")\n",
        "    similarity = similarity / np.max(similarity)\n",
        "\n",
        "    # return peaks (values in ms) using threshold\n",
        "    return ecg_signal[similarity > threshold].index, similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sO0w_0O085l"
      },
      "outputs": [],
      "source": [
        "def group_peaks(p, threshold=5):\n",
        "    '''\n",
        "    The peak detection algorithm finds multiple peaks for each QRS complex.\n",
        "    Here we group collections of peaks that are very near (within threshold) and we take the median index\n",
        "    '''\n",
        "    # initialize output\n",
        "    output = np.empty(0)\n",
        "\n",
        "    # label groups of sample that belong to the same peak\n",
        "    peak_groups, num_groups = label(np.diff(p) < threshold)\n",
        "\n",
        "    # iterate through groups and take the mean as peak index\n",
        "    for i in np.unique(peak_groups)[1:]:\n",
        "        peak_group = p[np.where(peak_groups == i)]\n",
        "        output = np.append(output, np.median(peak_group))\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW16tGB_5H12"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(x_g_AB.flatten(), columns = ['ECG'])\n",
        "df2 = pd.DataFrame(X_test.flatten(), columns = ['SCG'])\n",
        "df3 = pd.DataFrame(y_test.flatten(), columns = ['ECG_orig'])\n",
        "#df['column_name']=pd.Series(x_g_AB.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl2uDfgknydZ"
      },
      "outputs": [],
      "source": [
        "# n_samples = f.getNSamples()[0]\n",
        "\n",
        "# # Extract the signal data and flatten it\n",
        "# signal_data = f.readSignal(0).flatten()\n",
        "\n",
        "# # Close the file\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG1RQ-DoOSoy"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7cR2ZpfPWQW"
      },
      "outputs": [],
      "source": [
        "import wfdb\n",
        "from wfdb import processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u5zX8_aGcTm"
      },
      "outputs": [],
      "source": [
        "# to be used for wsdb to edf conversion and reading data using wsdb\n",
        "import wfdb\n",
        "from google.colab import files\n",
        "fileName_str\n",
        "record = wfdb.rdrecord('b001', pn_dir='cebsdb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tlWmRfQnsGt"
      },
      "outputs": [],
      "source": [
        "# # Get the number of channels and samples\n",
        "# n_channels = f.signals_in_file\n",
        "# n_samples = f.getNSamples()[0]\n",
        "\n",
        "# # Extract the signal data and flatten it\n",
        "# signal_data = f.readSignal(0).flatten()\n",
        "\n",
        "# # Close the file\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlovSfviNPG"
      },
      "outputs": [],
      "source": [
        "record.p_signal[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBWEowuanOEA"
      },
      "outputs": [],
      "source": [
        "# qrs = codeFn(ecg_signal, fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEjBAFgSPcVf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Demo 19 - Use the GQRS detection algorithm and correct the peaks\n",
        "\n",
        "def peaks_hr(sig, peak_inds, fs, title, figsize=(20, 10), saveto=None):\n",
        "    \"Plot a signal with its peaks and heart rate\"\n",
        "    # Calculate heart rate\n",
        "    hrs = processing.hr.compute_hr(sig_len=sig.shape[0], qrs_inds=peak_inds, fs=fs)\n",
        "    print(hrs)\n",
        "    N = sig.shape[0]\n",
        "\n",
        "    fig, ax_left = plt.subplots(figsize=figsize)\n",
        "    ax_right = ax_left.twinx()\n",
        "\n",
        "    ax_left.plot(sig, color='#3979f0', label='Signal')\n",
        "    ax_left.plot(peak_inds, sig[peak_inds], 'rx', marker='x',\n",
        "                 color='#8b0000', label='Peak', markersize=12)\n",
        "    ax_right.plot(np.arange(N), hrs, label='Heart rate', color='m', linewidth=2)\n",
        "\n",
        "    ax_left.set_title(title)\n",
        "\n",
        "    ax_left.set_xlabel('Time (ms)')\n",
        "    ax_left.set_ylabel('ECG (mV)', color='#3979f0')\n",
        "    ax_right.set_ylabel('Heart rate (bpm)', color='m')\n",
        "    # Make the y-axis label, ticks and tick labels match the line color.\n",
        "    ax_left.tick_params('y', colors='#3979f0')\n",
        "    ax_right.tick_params('y', colors='m')\n",
        "    if saveto is not None:\n",
        "        plt.savefig(saveto, dpi=600)\n",
        "    plt.show()\n",
        "    return hrs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ZCHPZXoCxJ"
      },
      "outputs": [],
      "source": [
        "# t = np.arange(len(ecg_signal)) / fs\n",
        "# plt.plot(t, ecg_signal)\n",
        "# plt.plot(qrs/fs, ecg_signal[qrs], 'ro')\n",
        "# plt.xlabel('Time (s)')\n",
        "# plt.ylabel('ECG Amplitude')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OajguMORasOE"
      },
      "outputs": [],
      "source": [
        "# Load the WFDB record and the physical samples\n",
        "#record = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0])\n",
        "\n",
        "# Use the GQRS algorithm to detect QRS locations in the first channel\n",
        "qrs_inds = processing.qrs.gqrs_detect(sig=record.p_signal[:,0], fs=record.fs)\n",
        "\n",
        "# Plot results\n",
        "hrs = peaks_hr(sig=record.p_signal, peak_inds=qrs_inds, fs=record.fs,\n",
        "         title=\"GQRS peak detection on record b001\")\n",
        "\n",
        "# Correct the peaks shifting them to local maxima\n",
        "min_bpm = 20\n",
        "max_bpm = 230\n",
        "#min_gap = record.fs * 60 / min_bpm\n",
        "# Use the maximum possible bpm as the search radius\n",
        "search_radius = int(record.fs * 60 / max_bpm)\n",
        "corrected_peak_inds = processing.peaks.correct_peaks(record.p_signal[:,0],\n",
        "                                                     peak_inds=qrs_inds,\n",
        "                                                     search_radius=search_radius,\n",
        "                                                     smooth_window_size=150)\n",
        "\n",
        "# Display results\n",
        "print('Corrected GQRS detected peak indices:', sorted(corrected_peak_inds))\n",
        "hrs = peaks_hr(sig=record.p_signal, peak_inds=sorted(corrected_peak_inds), fs=record.fs,\n",
        "         title=\"Corrected GQRS peak detection on sampledata/100\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVKKF0h-LaTR"
      },
      "outputs": [],
      "source": [
        "# Load the WFDB record and the physical samples\n",
        "#record = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0])\n",
        "\n",
        "# Use the GQRS algorithm to detect QRS locations in the first channel\n",
        "qrs_inds = processing.qrs.gqrs_detect(sig=record.p_signal[:,3], fs=record.fs)\n",
        "\n",
        "# Plot results\n",
        "hrs_scg = peaks_hr(sig=record.p_signal, peak_inds=qrs_inds, fs=record.fs,\n",
        "         title=\"GQRS peak detection on record b001\")\n",
        "\n",
        "# Correct the peaks shifting them to local maxima\n",
        "min_bpm = 20\n",
        "max_bpm = 230\n",
        "#min_gap = record.fs * 60 / min_bpm\n",
        "# Use the maximum possible bpm as the search radius\n",
        "search_radius = int(record.fs * 60 / max_bpm)\n",
        "corrected_peak_inds = processing.peaks.correct_peaks(record.p_signal[:,3],\n",
        "                                                     peak_inds=qrs_inds,\n",
        "                                                     search_radius=search_radius,\n",
        "                                                     smooth_window_size=150)\n",
        "\n",
        "# Display results\n",
        "print('Corrected GQRS detected peak indices:', sorted(corrected_peak_inds))\n",
        "hrs_scg = peaks_hr(sig=record.p_signal, peak_inds=sorted(corrected_peak_inds), fs=record.fs,\n",
        "         title=\"Corrected GQRS peak detection on sampledata/100\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyrnU2HEV__r"
      },
      "outputs": [],
      "source": [
        "len(hrs)\n",
        "hrs = hrs[~np.isnan(hrs)]\n",
        "hrs=hrs[hrs<160]\n",
        "#hrs=hrs[hrs>100]\n",
        "print(len(hrs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DhtZiNIWGSP"
      },
      "outputs": [],
      "source": [
        "len(hrs_scg)\n",
        "\n",
        "hrs_scg = hrs_scg[~np.isnan(hrs_scg)]\n",
        "hrs_scg=hrs_scg[hrs_scg<160]\n",
        "#hrs_scg=hrs_scg[hrs_scg>100]\n",
        "print(len(hrs_scg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HsDwqZID_xZ"
      },
      "outputs": [],
      "source": [
        "# plotting the data\n",
        "plt.scatter(hrs[:len(hrs_scg)], hrs_scg)\n",
        "\n",
        "# This will fit the best line into the graph\n",
        "plt.plot(np.unique(hrs), np.poly1d(np.polyfit(hrs[:len(hrs_scg)], hrs_scg, 1))\n",
        "         (np.unique(hrs)), color='red')\n",
        "plt.ylabel(\"SCG heart rate\")\n",
        "plt.xlabel(\"ECG heart rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvedI0cjGOmL"
      },
      "outputs": [],
      "source": [
        "!pip install pyhrv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0PTfIEd1MGa"
      },
      "outputs": [],
      "source": [
        "# detect peaks\n",
        "print(len(df.ECG))\n",
        "print(len(df2.SCG))\n",
        "print(len(df3.ECG_orig))\n",
        "peaks_ecg_pred, similarity = detect_peaks(df.ECG, threshold=0.3)\n",
        "peaks_scg, similarity = detect_peaks(df2.SCG, threshold=0.3)\n",
        "peaks_ecg, similarity = detect_peaks(df3.ECG_orig, threshold=0.3)\n",
        "\n",
        "print(len(peaks_ecg_pred))\n",
        "print(len(peaks_scg))\n",
        "print(len(peaks_ecg))\n",
        "# group peaks so we get a single peak per beat (hopefully)\n",
        "\n",
        "from scipy.ndimage import label\n",
        "grouped_peaks_ecg_pred = group_peaks(peaks_ecg_pred)\n",
        "grouped_peaks_scg = group_peaks(peaks_scg)\n",
        "grouped_peaks_ecg = group_peaks(peaks_ecg)\n",
        "\n",
        "print(len(grouped_peaks_ecg_pred))\n",
        "print(len(grouped_peaks_scg))\n",
        "print(len(grouped_peaks_ecg))\n",
        "# RR-intervals are the differences between successive peaks\n",
        "rr_ecg_pred = np.diff(grouped_peaks_ecg_pred)\n",
        "rr_scg = np.diff(grouped_peaks_scg)\n",
        "rr_ecg = np.diff(grouped_peaks_ecg)\n",
        "\n",
        "print(len(rr_ecg_pred))\n",
        "print(len(rr_scg))\n",
        "print(len(rr_ecg))\n",
        "# plot RR-intervals\n",
        "#plt.figure(figsize=(20, 7))\n",
        "#plt.title(\"RR-intervals\")\n",
        "#plt.xlabel(\"Time (ms)\")\n",
        "#plt.ylabel(\"RR-interval (ms)\")\n",
        "\n",
        "#plt.plot(np.cumsum(rr), rr, label=\"RR-interval\", color=\"#A651D8\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdicn0_cI5tj"
      },
      "outputs": [],
      "source": [
        "from matplotlib.patches import Ellipse\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM98N4hTIhbL"
      },
      "outputs": [],
      "source": [
        "def plot_poincare(rr):\n",
        "    rr_n = rr[:-1]\n",
        "    rr_n1 = rr[1:]\n",
        "\n",
        "    sd1 = np.sqrt(0.5) * np.std(rr_n1 - rr_n)\n",
        "    sd2 = np.sqrt(0.5) * np.std(rr_n1 + rr_n)\n",
        "\n",
        "    m = np.mean(rr)\n",
        "    min_rr = np.min(rr)\n",
        "    max_rr = np.max(rr)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.title(\"Poincare plot\")\n",
        "\n",
        "    sns.scatterplot(x=rr_n, y=rr_n1, color=\"#51A6D8\")\n",
        "\n",
        "    plt.xlabel(r'$RR_n (ms)$')\n",
        "    plt.ylabel(r'$RR_{n+1} (ms)$')\n",
        "\n",
        "    e1 = Ellipse((m, m), 2*sd1, 2*sd2, angle=-45, linewidth=1.2, fill=False, color=\"k\")\n",
        "    plt.gca().add_patch(e1)\n",
        "\n",
        "    plt.arrow(m, m, (max_rr-min_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n",
        "    plt.arrow(m, m, (min_rr-max_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n",
        "\n",
        "    plt.arrow(m, m, sd2 * np.sqrt(0.5), sd2 * np.sqrt(0.5), color=\"green\", linewidth=5)\n",
        "    plt.arrow(m, m, -sd1 * np.sqrt(0.5), sd1 * np.sqrt(0.5), color=\"red\", linewidth=5)\n",
        "\n",
        "    plt.text(max_rr, max_rr, \"SD2\", fontsize=20, color=\"green\")\n",
        "    plt.text(m-(max_rr-min_rr)*0.4-20, max_rr, \"SD1\", fontsize=20, color=\"red\")\n",
        "\n",
        "    return sd1, sd2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XtMdphUInOn"
      },
      "outputs": [],
      "source": [
        "sd1, sd2 = plot_poincare(rr_ecg_pred)\n",
        "print(\"SD1: %.3f ms\" % sd1)\n",
        "print(\"SD2: %.3f ms\" % sd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9RogA2aBkBl"
      },
      "outputs": [],
      "source": [
        "sd1, sd2 = plot_poincare(rr_scg)\n",
        "print(\"SD1: %.3f ms\" % sd1)\n",
        "print(\"SD2: %.3f ms\" % sd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcMUKpkBGcVP"
      },
      "outputs": [],
      "source": [
        "import pyhrv\n",
        "pyhrv.nonlinear.poincare(rr_ecg_pred,peaks_ecg_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq7uWKlkv5YL"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "true = y_test.astype('float')\n",
        "pre =  x_g_AB.astype('float')\n",
        "corr, pval =pearsonr(true.flatten(),pre.flatten())\n",
        "print(\"corre and p-val:: \",corr, pval)\n",
        "\n",
        "\n",
        "pre_smoothed =  x_g_AB_smoothed.astype('float')\n",
        "corr_sm, pval_sm =pearsonr(true.flatten(),pre_smoothed.flatten())\n",
        "print(\"smoothed corre and p-val:: \",corr_sm, pval_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzA3hVXVKqvG"
      },
      "outputs": [],
      "source": [
        "accuracy_for_AtoB = 100 - (np.mean(np.abs(x_g_AB-y_test)*100))\n",
        "print(accuracy_for_AtoB)\n",
        "\n",
        "accuracy_for_AtoB_sm = 100 - (np.mean(np.abs(x_g_AB_smoothed-y_test)*100))\n",
        "print(\"smoothed\",accuracy_for_AtoB_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zJcNkEDIeWj"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.dpi'] = 300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiGoXcO2HLNl"
      },
      "outputs": [],
      "source": [
        "print(len(str(fileNames[0]))-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rz4ZFn5UxcS"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_first_5000.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqQmbihI3VFj"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# # plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_blue.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nOF_7Vg5p6z"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# # plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_red.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccpSirZgbL7O"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_first_5000_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9BydaDI51Dr"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# # plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_blue_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwXrm5MH6Idu"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# fig = plt.figure(figsize=(4,3))\n",
        "# fig.set_size_inches(50,7)\n",
        "# # plt.plot(y_test.flatten()[:5000],color=\"blue\")\n",
        "# plt.plot(np.array(x_g_AB_smoothed).flatten()[:5000], color=\"red\")\n",
        "# plt.show()\n",
        "# fig.savefig('/content/drive/MyDrive/Colab Notebooks/basal/b_to_b_'+str(fileNames[0])[0:len(str(fileNames[0]))-4]+'_red_smoothed.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSjZzZUsVD_x"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(y_test)):\n",
        "#   fig = plt.figure(figsize=(4,3))\n",
        "#   fig.set_size_inches(50,7)\n",
        "#   plt.plot(y_test[i],color=\"blue\")\n",
        "#   plt.plot(x_g_AB[i], color=\"red\")\n",
        "#   plt.show()\n",
        "#   plt.savefig('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/b_to_b_1_batch_'+str(i+1)+'.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mb0GpQobaAR"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(y_test)):\n",
        "#   fig = plt.figure(figsize=(4,3))\n",
        "#   fig.set_size_inches(50,7)\n",
        "#   plt.plot(y_test[i],color=\"blue\")\n",
        "#   plt.plot(x_g_AB_smoothed[i], color=\"red\")\n",
        "#   plt.savefig('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/b_to_b_1_smoothed_batch_'+str(i+1)+'.jpg')\n",
        "#   plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R4embaispm9"
      },
      "outputs": [],
      "source": [
        "# !pip install frechetdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsD1i3QNcs_a"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# print(sys.getrecursionlimit())\n",
        "# sys.setrecursionlimit(5000)\n",
        "# print(sys.getrecursionlimit())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtiJpcXEnjc6"
      },
      "outputs": [],
      "source": [
        "# from frechetdist import frdist\n",
        "# fd = []\n",
        "# for i in range(len(y_test)):\n",
        "#   fd.append(frdist(y_test[i],x_g_AB[i]))\n",
        "# print(\"FD: \", np.array(fd).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mapGaw0HvhHa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#mmd_arr = mmd(y_test,x_g_AB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FME3FqsWtliD"
      },
      "outputs": [],
      "source": [
        "# fd_mean = np.array(fd).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqyCpTSVntXV"
      },
      "outputs": [],
      "source": [
        "#To create the blank csv with header.\n",
        "\n",
        "#from csv import writer\n",
        "\n",
        "#List=[\"Subject\",\"Generator param A\", \"Generator param B\",\"Reconstructor param A\",\"Reconstructor param B\",\"RMSE flattened\", \"RMSE Smoothed flattened\",\"RMSE All Mean\", \"RMSE All Smoothed Mean\", \"prd\", \"prd smoothed\",\"mae\",\"mae smoothed\", \"correlation\",\"correlation smoothed\",\"p-val\",\"p-val smoothed\",\"accuracy\",\"accuracy smoothed\",\"frachet dist\"]\n",
        "\n",
        "#with open('/content/drive/MyDrive/Colab Notebooks/GAN_PAPER_DATA/basal/matrices.csv', 'w') as f_object:\n",
        "\n",
        "#    writer_object = writer(f_object)\n",
        "#    writer_object.writerow(List)\n",
        "\n",
        "#    f_object.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#changed\n",
        "# Assuming y_test is your ground truth and x_g_AB is your predicted data\n",
        "y_test_features = []\n",
        "x_g_AB_features = []\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_test_features.append(extract_fft_features(y_test[i].flatten()))\n",
        "    x_g_AB_features.append(extract_fft_features(x_g_AB[i].flatten()))\n",
        "\n",
        "y_test_features = np.array(y_test_features)\n",
        "x_g_AB_features = np.array(x_g_AB_features)\n"
      ],
      "metadata": {
        "id": "gp431JmXPqyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changed\n",
        "# Performance evaluation with FFT features\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse_fft = mean_squared_error(y_test_features.flatten(), x_g_AB_features.flatten())\n",
        "mae_fft = mean_absolute_error(y_test_features.flatten(), x_g_AB_features.flatten())\n",
        "\n",
        "print(f'MSE with FFT features: {mse_fft}')\n",
        "print(f'MAE with FFT features: {mae_fft}')\n"
      ],
      "metadata": {
        "id": "99BVG3ffPypo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tBmFw4I12-y"
      },
      "outputs": [],
      "source": [
        "# change the parameters here\n",
        "gen_param_a = 0\n",
        "gen_param_b = 0\n",
        "rec_param_a = 0\n",
        "rec_param_b = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW92FBJMs1rq"
      },
      "outputs": [],
      "source": [
        "# List\n",
        "List_vals =[fileNames[0], gen_param_a,gen_param_b,rec_param_a,rec_param_b, rmse_arr, rmse_smoothed, rmse_all_mean, rmse_all_smoothed_mean, prd_arr, prd_arr_smoothed,mae_arr,mae_arr_smoothed, corr,corr_sm,pval,pval_sm,accuracy_for_AtoB,accuracy_for_AtoB_sm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYk_0-G-t3sX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from csv import writer\n",
        "\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/basal/matrices.csv', 'a') as f_object:\n",
        "\n",
        "#     writer_object = writer(f_object)\n",
        "#     writer_object.writerow(List_vals)\n",
        "\n",
        "#     f_object.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSqpdHYCJA8y"
      },
      "outputs": [],
      "source": [
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Xg6pwAiY4h"
      },
      "outputs": [],
      "source": [
        "# !pip install -c dloewenstein torch-two-sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aH-uFRmJZm5"
      },
      "outputs": [],
      "source": [
        "#!pip install mmd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90eindpKZ7G"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S733gBm7Qrx5"
      },
      "outputs": [],
      "source": [
        "# from torch_two_sample.statistics_diff import MMDStatistic\n",
        "# print(mmd(y_test,x_g_AB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X75GyDycKd0j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV3DzxzNGsdJ"
      },
      "outputs": [],
      "source": [
        "#!pip install ecg_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPx2jXZawBbP"
      },
      "outputs": [],
      "source": [
        "#import ecg_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnC5ZOPVo6yi"
      },
      "outputs": [],
      "source": [
        "#for i in range(10):\n",
        "#  ecg_plot.plot_1(y_test_scaled[i], sample_rate=500, title = 'Original ECG')\n",
        " # ecg_plot.plot_1(x_g_AB_scaled[i], sample_rate=500, title = 'Predicted ECG')\n",
        "  #plt.tight_layout()\n",
        " # ecg_plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt7WsGomHyAD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#ecg_plot.plot_1(y_test.flatten(), sample_rate=1000, title = 'Original')\n",
        "#ecg_plot.plot_1(x_g_AB.flatten(), sample_rate=1000, title = 'Predicted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNUu09O8WhX3"
      },
      "outputs": [],
      "source": [
        "#x = y_test[0].flatten().tolist()\n",
        "#y = x_g_AB[0].flatten().tolist()\n",
        "#for i in range(len(x_g_AB)):\n",
        "#  x.append((y_test[i][len(y_test[i])-1])[0])\n",
        "#  y.append((x_g_AB[i][len(x_g_AB[i])-1])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7ofa2xOc49"
      },
      "outputs": [],
      "source": [
        "print(\"acc: %3d%%\"% accuracy_for_AtoB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux9A5BOdPNZD"
      },
      "outputs": [],
      "source": [
        "# normalize correlation\n",
        "a = (y_test - np.mean(y_test)) / (np.std(y_test) * len(y_test))\n",
        "b = (x_g_AB - np.mean(x_g_AB)) / (np.std(x_g_AB))\n",
        "c = np.correlate(a.flatten(), b.flatten(), 'full')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVfrUH2l_X3o"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test.flatten(), x_g_AB.flatten())\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SuBwpYd_oO_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y_test.flatten(), x_g_AB.flatten())\n",
        "print(mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE1VHpvD0-AH"
      },
      "outputs": [],
      "source": [
        "y_pred = x_g_AB.flatten()\n",
        "y_test_new = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs64HmthxRFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred[y_pred > 0.5] = 1\n",
        "y_pred[y_pred <= 0.5] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn6Cv1P9xo49"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_test_new[y_test_new > 0.5] = 1\n",
        "y_test_new[y_test_new <= 0.5] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2cFEBwO_8Wo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ailUHhBooDXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"accuracy_score\",accuracy_score(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwTBJos0zUAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"precision_score\",precision_score(y_test_new, y_pred , pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUaN4GV2xig6"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"f1_score\",f1_score(y_test_new, y_pred, pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxidi8ZXxlpA"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"recall_score\",recall_score(y_test_new, y_pred, pos_label='positive', average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAC5ojYUxnmy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "a = [1,1,1,1,1,0]\n",
        "b=[1,1,1,1,0,0]\n",
        "matrix = confusion_matrix(y_test_new, y_pred)\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BIF5Y8gC2-v"
      },
      "outputs": [],
      "source": [
        "# correlation coefficient r of x and y\n",
        "from scipy import stats\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(y_test_new, y_pred)\n",
        "print (r_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnqyx3KsDERn"
      },
      "outputs": [],
      "source": [
        "# The coefficient of determination\n",
        "from sklearn.metrics import r2_score\n",
        "print(r2_score(y_test_new, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWtT4BLEvIQK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}